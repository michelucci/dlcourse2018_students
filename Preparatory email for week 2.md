Hallo Everyone,
the second introductory week material is now available on the repository, please download it before Tuesday. In case you have questions on the first week material don't hesitate to contact me.
In case you need the link, here it is: https://github.com/michelucci/dlcourse2018_students 

Since you are all very well prepared, I thought we could cover some very interesting and exciting material next week.

Our main goal Tuesday will be to get a good understanding of what an optimization problem is (for us finding the minimum or maximum of a multi dimensional function) and what algorithms we can use. We will look at two: the Nelder Mead (NM) and the Gradient Descent (GD). We will use the GD for all the weeks in the course, so understanding it is _very_ important. Understanding those two will also let you understand the learning rate and its quirks. The learning rate is _the_ most important hyper-parameter that you will find when training neural networks, and understanding it well is very important.

NOTE: for those not there, we will look at the GD algorithm again, so don't worry. To understand the rest of the course you will not need the NM algorithm. I will explain the basics of the GD algorithm quickly again when we will start using it if needed.

Additionally we will look at how important is to look at the data that we will feed to our networks, and we will build a classifier just studying the data we have.

Important
As I told you in class, is very important for me to understand if the speed and the difficulty are right for you. So please let me know (in class, by mail, calling me, as you like) if I am going to fast, to slow, if you need more help with Python, with math or whatever. This course is for you. The worst that can happen is that you don't profit from it. I am happy to give you additional material or meet you a bit earlier or on another day to help you if you need. Just let me know ;-)

Looking forward to meeting you on Tuesday,
Umberto
